\documentclass[12pt,a4paper]{article}
\usepackage{times}
\usepackage{durhampaper}
\usepackage{harvard}
\usepackage{float}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgf-pie}
\citationmode{abbr}
\bibliographystyle{agsm}
\title{Image Processing using JavaScript and Web Assembly}
\author{} % leave; your name goes into \student{}
\student{Sam Robbins}
\supervisor{Tom Friedetzky}
\degree{BSc Computer Science}

\date{}

\begin{document}

\maketitle

\begin{abstract}
    \subsection{Context/Background}
    Web Assembly is a relatively new method for computation on the web, allowing for the use of a much wider range of languages. This is being used in high intensity contexts to improve computation time.

    \subsection{Aims}

    To determine in which cases implementing algorithms in Web Assembly is the right choice to make

    \subsection{Method}

    Implement a range of image processing algorithms in both JavaScript and Web Assembly and measure using a variety of metrics.

    \subsection{Results}

    An improvement of web assembly compared to JavaScript of over $8\times$ in some tasks, however demonstrating that some tasks and contexts are less suitable for the use of Web Assembly.

    \subsection{Conclusions}

    Web Assembly has the capability for providing large increases in performance, however it is not a magic bullet for performance, and the individual task needs considering before using it.
\end{abstract}

\begin{keywords}
    Web Assembly, JavaScript, Image Processing
\end{keywords}


\section{Introduction}

Image processing is a widely used technique for a range of problems. In most modern cameras there will be some aspect of image processing at the time of capture, whether it is enhancements to the image or just the formatting to a file type, such as JPEG. After capture there is often additional image processing, be it applying filters or compression for upload online. Sophisticated image processing techniques don't just have benefit for the sharing of photos, but are also used medically, such as for processing CT images \cite{zhang2017applications}.

The performance of image processing was greatly improved with the introduction of Digital Signal Processors (DSP), these are specialized chips for performing signal processing tasks, such as the discrete cosine transform. In mobile devices these are now often integrated into the System on a Chip (SOC) \cite{angoletta2008digital}, however with the increase in computational performance since DSPs were introduced, these algorithms can also be run on the main processor.

Traditionally image processing has been done in applications like Adobe Photoshop, however as browser market share is increasing web based photo editors are becoming more common. In a study by Forrester Consulting, workers are spending 1/3 of the work day on average in a web browser, so it is an area many companies are targeting to launch new products \cite{cloud_worker}.

% Look more into is it a good idea, not just if it's used

In 1995 JavaScript was introduced as a method for introducing interactivity to web applications, since then there were some improvements made for programmers who wanted to perform computationally intensive tasks with their website, such as Web Workers, introduced in 2009 \cite{Hickson}. However, one of the biggest steps forward is the introduction of Web Assembly in 2017, allowing for assembly code to be executed by web browsers \cite{haas2017bringing}.

Web Assembly is a compilation target for languages that support it, meaning that you take the code in the language of your choice and compile it into a \texttt{.wasm} file. This file can then be called using JavaScript to make it behave like a standard JavaScript library.

Using Web Assembly over JavaScript proposes advantages such as being able to use your existing codebase, rather than having to translate it into JavaScript along with benefits such as type safety using static types, where JavaScript uses dynamic types. TypeScript works to try and solve this problem, but is just a transpiler to JavaScript, where Web Assembly allows for type checking at runtime.

When it comes to producing a web application, the computation can be run on either the client (web browser) or the server, each coming with advantages and disadvantages. Running code on the server allows for the site to be performant on low power devices such as mobile phones, however by running code on the client you don't need to send data between the client and server on every interaction. This allows the site to continue being ran during times of intermittent connection and reduces the latency of operations. Running code on the client also allows for the site to be turned into a progressive web app, which only need to be connected to the internet once for installation similarly to an app \cite{biorn2017progressive}.






\subsection{Objectives}

This project will implement a range of image processing algorithms in both Web Assembly and JavaScript and compare their performance.


\subsection{Research Question}

The Research question is to find in which cases Web Assembly offers a benefit over JavaScript, finding this out by implementing image processing algorithms using both mechanisms.





\section{Related Work}

\subsection{Language options for Web Assembly}

Web Assembly is compatible with any language that compiles to assembly, meaning there is a large range of languages to choose from. However, as Web Assembly doesn't currently have a garbage collector, it is most efficient to choose a language that doesn't do garbage collection, such as C \cite{haas2017bringing}. Alongside C, Rust is another language that doesn't have garbage collection and advertises itself as good to use for Web Assembly.

One advantage of Rust is that it has a large ecosystem of packages to help with the production of Web Assembly code. One such library is wasm-bindgen which allows for binding functions and other features in Rust to the JavaScript produced, allowing for multiple functions to be included in one Web Assembly file \cite{wasmbindgen}. It also provides availability for using native browser functions, such as \texttt{window.alert}, within your Rust functions. Another is wasm-pack which creates a JavaScript library containing the generated Web Assembly code, making it easier to use in code and publish.

% Might need to discuss what garbage collection is
% Garbage collection in WASM https://github.com/WebAssembly/proposals/issues/16
% WASM-bindgen

\subsection{Important metrics}

There are a range of metrics that are important when evaluating the performance of web tools. Response time is one such metric, and measures the time it takes to receive feedback from an operation. This applies for both the time it takes for a page to load and when doing something on a webpage. Research by Neilson discovered that \cite{nielsen1994usability}:
\begin{itemize}
    \item 0.1 second is the limit for instantaneous feeling
    \item 1.0 seconds is the limit for the user's flow of thought to remain uninterrupted
    \item 10 seconds is the limit for keeping the user's attention.
\end{itemize}

Making these results actionable, Google introduced Core Web Vitals, which are metrics about how fast a page loads to determine how well it performs \cite{webvitals}. Largest Contentful Paint measures the time taken for most of the page to be loaded in, and suggests that it should occur within 2.5 seconds to be classified as “Good”.




\subsection{Web Assembly Image Processing in use}

Squoosh by Google Chrome is a tool to compress images and implements many of its codecs using Web Assembly, this approach was also adopted by Next.js for their image component to improve performance, reducing the installed size by 27.3 MB from a total installed size of 96.5 MB \cite{nextjs}. As this was replacing a package with code in the project, it led to a large increase in the amount of code to maintain, but this could be abstracted to a package. One example of such a package is \texttt{photon} which provides abstractions on top of the \texttt{image} Rust library \cite{photon}.

% Total installed size from https://nextjs.org/blog/next-10-1



\subsection{Performance of Web Assembly}

Alongside image processing, Web Assembly is also used for a range of other purposes. Figma is a tool for designers, and implemented Web Assembly to improve their load times 3$\times$ compared to their previous solution which translated assembly to JavaScript \cite{figmawasm}.

Another implementation of Web Assembly is in determining the quality of DNA sequencing data, as used by fastq.bio \cite{fastq}. In this implementation they achieved a $9\times $ performance improvement, managing to increase that to $21 \times$ with some additional optimizations such as using a single function call.


However, when running code natively, Web Assembly may not be the best solution. A JavaScript bundler is a program which creates large files from the smaller JavaScript files needed for a web application, this is useful as better performance can be achieved by sending a few large files compared to lots of smaller files. The author of a native JavaScript bundler observed that there was a 10$\times$ speed improvement by running it natively, compared to using Web Assembly \cite{esbuild}.

This behaviour was also observed in \cite{jangda2019not}, where peak slowdowns were observed of 2.5$\times$ in Google Chrome compared to the native code. These benchmarks are taken from SPEC CPU2006 and SPEC CPU2017. A sample of these results are shown in Table \ref{native}.
\begin{table}[H]
    \centering
    \caption{Comparison of algorithms between native and wasm}
    \vspace*{6pt}
    \label{native}
    \begin{tabular}{cccc}\hline\hline
        Benchmark & Field                   & Native execution time & Google Chrome execution time \\ \hline
        bzip2     & Compression             & 370                   & 864                          \\
        mcf       & Combinatorial           & 221                   & 180                          \\
        milc      & Chromodynamics          & 375                   & 369                          \\
        namd      & Molecular Dynamics      & 271                   & 369                          \\
        gobmk     & Artificial Intelligence & 352                   & 537
    \end{tabular}
\end{table}

As one can see from these results, Native isn't guaranteed to be faster than Web Assembly, however this is often the case.



\subsection{Current Image Processing Techniques}

One of the most popular image processing libraries for JavaScript is Sharp with over 1,700,000 weekly downloads, this uses native code in the form of libvips and claims to be the fastest module for resizing. A slightly less popular library is jimp with over 1,400,000 weekly downloads, the difference with this library is that it is written entirely in JavaScript and so can be run in the browser. The difference between these and other libraries was measured on a task of resizing an image \cite{sharp}. These results are shown in Table \ref{imgproc}.

\begin{table}[htb]
    \centering
    \caption{Performance of Image Processing Libraries}
    \vspace*{6pt}
    \label{imgproc}
    \begin{tabular}{ccc}\hline\hline
        Library     & Best ops/sec & Best speedup \\ \hline
        jimp        & 0.77         & 1.0          \\
        mapnik      & 3.39         & 4.4          \\
        gm          & 4.33         & 5.6          \\
        imagemagick & 4.39         & 5.7          \\
        sharp       & 25.60        & 33.2
    \end{tabular}
\end{table}

% Add description of what the table headings mean

This shows native libraries being significantly faster than the solution in pure JavaScript, which means that even with the slowdown of Web Assembly compared to native code as discussed before, Web Assembly should still outperform JavaScript in this regard.

\subsection{Image formats}

There are a range of image formats in use, and the encoding and decoding of images is a computationally intensive task. The Web Almanac studies over 7 million websites and found that 40.26\% of images are of the jpg image format and 26.90\% in png \cite{webalmanac}. JPEG is a high performance image codec with a study by Cloudinary showing to to have an encoding speed of 49 MP/s and a decoding speed of 108 MP/s \cite{cloudinary}. JPEG has both lossless and lossy versions, and in a study of lossless compression of 382 images, these results are shown in Table \ref{speedratio} \cite{ukrit2011survey}.

% Add more details of what hardware this was one
% Don't include numbers if you don't have comparison

\begin{table}[H]
    \centering
    \caption{Compression speed and ratio for various algorithms}
    \vspace*{6pt}
    \label{speedratio}
    \begin{tabular}{ccc}\hline\hline
        Algorithm     & Compression Speed & Compression Ratio \\ \hline
        Lossless JPEG & 11.9              & 3.04              \\
        JPEG-LS       & 19.6              & 4.21              \\
        JPEG 2000     & 4.0               & 3.79              \\
        PNG           & 3.6               & 3.35              \\
    \end{tabular}
\end{table}

As one can see from these results, the compression ratio is very comparable between the various implementations of JPEG and PNG, however the compression speed greatly differs, with some JPEG algorithms significantly outperforming PNG.


\section{Solution}

\subsection{Platform}

Web Assembly can run in a wide variety of places, there are the different browsers, Chrome, Firefox and Safari, along with different platforms, web and mobile. Each of these will implement their processing of both JavaScript and Web Assembly differently, which makes choosing the platform an important topic. Google Chrome has a majority market share, making it a good choice as the results are most widely applicable \cite{webmarketshare}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \pie
        {64/Google Chrome,
            19/Safari, 4/Firefox, 13/Others}
    \end{tikzpicture}
    \caption{Browser Market Share}

\end{figure}
While the browser itself is important, the JavaScript engine is what will impact the results. This is the computer program responsible for executing JavaScript and Web Assembly code. The JavaScript engine inside Google Chrome is called V8, and is open source, allowing for other projects to use it. One such project is Node.js which allows for running JavaScript from the command line. This has become incredibly popular for the construction of web backends, with over 78,000 stars on GitHub. The benefit of using this over a browser is that the metrics will still be transferable to browsers, and allows me to reduce the number of external factors that impact the computation. An example of this is that a browser will manage the resources available to each of its tabs, meaning that having more tabs open could affect the result, whereas the resources of Node.js are managed by the operating system. This also makes it easier to collect metrics such as memory usage and timing and the process runs as a system process.

\subsection{Use of existing libraries}

In order to speed up the development of areas whose performance I'm not measuring, I use existing image processing libraries to do some of the work. For example in decoding a provided image into the simple r,g,b values so that I can then run an encoder on them.

For Rust these libraries are referred to as “crates” and for JavaScript these are provided as npm libraries. Rust is both a newer language, and less popular than JavaScript, meaning that it has a smaller range of available libraries, so when it came to choosing a crate, I only really had one choice, the \texttt{image} crate.

With JavaScript there is more choice, allowing me to choose the one that had the features I wanted, I settled on Jimp as it has a wide range of features and is very popular.

% Be more specific about what the image processing libraries are doing

% There's an opportunity to discuss other JS image processing solutions if I wanted to.
% Might be useful to have statistics to back this up, but I don't want to add too many citations

\subsection{Choice of algorithms}

There is a wide range of algorithms to choose from when it comes to image processing. The algorithms have been chosen to ensure they cover a wide range of performance characteristics. Some of the algorithms require a large amount of processing and are expected to take over a second, whereas others just manipulate each pixel in turn and so are expected to complete much faster. This is to provide a full picture of if there is a certain type of algorithm that benefits from one of the two approaches.

When editing photos, the operation that tends to take the most time is exporting the images to JPEG, so I decided to explore implementing the JPEG algorithm \cite{jpeg}.

\subsection{Implementation of JPEG}

For both implementations, I started with prewritten code to decode an image into a long string of r,g,b,a values and the width and height of the image. The first step is the transformation of the image from the r,g,b colour space into the YCbCr colour space. Doing this is the simple task of calculating fractions based on the r,g and b values, so is not especially intensive.

Next is downsampling; this isn't required by the JPEG standard, but can be used to further reduce image size. This reduces the resolution of the Cb and Cr components. It is done on these components as brightness (Y) is much more noticeable to the human eye compared to the other components, meaning the resolution can be reduced without a noticeable decrease in quality. To do this, each $2\times 2$ pixel area is averaged to a single pixel. On the borders where it isn't possible to get this area, the average is just calculated based on the available pixels.

For the next part of the algorithm, each component is split into $8\times 8$ blocks. This exhibits the same problem as the previous step as at the edges a full $8\times 8$ block may not be possible. My implementation solves this by replacing the pixels with black, however other implementations may use an average of the existing pixels to ensure there is less influence on the image.

The next step, the discrete cosine transform, is the most intensive step of the whole algorithm. The simplest way to implement this is to multiply each $8\times 8$ matrix by another $8\times 8$ matrix. For each entry in the resulting matrix there will be $8$ multiplications. In my implementation I use the naive implementation of Matrix multiplication, giving $8^3 = 512$ multiplications in total per block. There is an opportunity to improve the number of multiplications required here using Strassen's algorithm, which would use $8^{\log_2(7)}=343$ multiplications \cite{strassen1969gaussian}. However, it has been found that this algorithm is only better than the naive method for large matrices, and so on an $8\times 8$ matrix, the naive algorithm has better performance \cite{huang2016strassen}.


% The design of this transform results in high absolute values in the top left of the matrix, with low absolute values in the bottom right.

The next step is quantization, this is a lossy operation to keep only the lower frequency data as the human eye struggles to distinguish the strength of high frequency brightness variation. In order to apply quantization, for each element in the matrix, it is divided by the value in the same position of a quantization matrix and rounded. JPEG has a variety of quantization matrices, allowing for the user to specify their desired quality, with the matrices having lower values for higher qualities. Implementing the quantization matrix will lead to many of the elements being zero, allowing for easier encoding.

This encoding begins with run length encoding, intended for allowing for the encoding of runs of zeroes. This algorithm uses “zigzag encoding” in which the order in which the elements are processed is defined by a zigzag pattern, shown in the image below, taken from \cite{jpeg}.

\begin{figure}[H]
    \centering
    \includegraphics{zigzag.png}
    \caption{Zigzag order}
\end{figure}

This then forms a sequence of values, ideally with many zeros at the end. As the length is something that is already known, the encoding can stop when the rest of the sequence is just zeroes. The most common encoding to use for this is Huffman, however the specification does allow for using arithmetic coding.

\subsubsection{Writing to the file}

The remainder of the algorithm involves correctly writing the data to a file. For this the source code for the JPEG encoders in the libraries discussed previously used, as this work isn't computationally intensive, but requires a lot of detail in order to ensure it is done correctly.

\subsection{Gaussian Blur}

Gaussian blur has a wide variety of applications, but is particularly useful for image denoising. The blurring effect helps to reduce noise artefacts caused by low light, and this is particularly useful in low end cameras such as mobile phones due to their small sensor size leading to poor performance in low light.


For the Gaussian Blur implementation, I have used the built in implementations available in jimp and the image crate. These behave in a very similar way and so are suitable for comparison, and the same values are used for both implementation.

\subsection{Brightness and Contrast}

Brightness and contrast are two operations that just perform a defined action on each pixel, this means that the implementations in the different libraries will be doing exactly the same thing.

\subsection{Web Assembly optimizations}

JavaScript comes with no options for optimizing performance, as it is run by the browser as JavaScript. However, as Rust is first compiled to assembly, it offers a range of ways to trade off between size and speed, and some methods that have the potential to improve both.

The first optimization is Link-time optimization (LTO), this takes the intermediate representation of the code, allowing the whole codebase to be optimized together, rather than optimizing each file individually. Doing this can help to improve both the size and performance of the code.

The rust compiler then has an “opt-level”, which allows the user to specify how far they are willing to optimize size at the detriment of performance, this has two options, \texttt{s} and \texttt{z}, with z being the most optimized for size.

Finally, there is the \texttt{wasm-opt} tool, which allows for optimizing a \texttt{.wasm} file after it has been created, to optimize for either speed or size. It provides the following options

\begin{itemize}
    \item \texttt{-Os} : optimize for size
    \item \texttt{-Oz} : optimize aggressively for size
    \item \texttt{-O} : optimize for speed
    \item \texttt{-O3} : optimize aggressively for speed
\end{itemize}

\subsubsection{Results}

The performance of the various options are to be evaluated in order to determine what should be used when collecting the results, as both JavaScript and Web Assembly should be performing at the best they can be made. For this experiment, the Gaussian blur method has been used as it is computationally intensive.

For the Web Assembly optimizations, the measurements are based on the Gaussian blur task. The first measurement to take is how long each process takes.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={No changes, LTO, Opt-s, Opt-z, -Os, -Oz, -O, -O3},
                xtick=data,
                x tick label style={rotate=45, anchor=north east, inner sep=0mm},
                bar width = 20,
                enlarge x limits = 0.1,
                ybar,
                ylabel = Time(Milliseconds),
                y label style={at={(axis description cs:-0.01,.5)},anchor=south},
                ymin=0,
                nodes near coords,
                axis x line*=bottom,
                axis y line*=left,
                width=400,
                height=200		]
            \addplot coordinates {
                    (No changes,  1031)
                    (LTO, 846)
                    (Opt-s, 2275)
                    (Opt-z, 3846)
                    (-Os, 1078)
                    (-Oz, 1087)
                    (-O, 1154)
                    (-O3, 1028)
                };
        \end{axis}
    \end{tikzpicture}
    \caption{Processing time of Web Assembly optimizations}
\end{figure}

These measurements need to be contrasted with the file sizes they produce to fully evaluate the benefit of each optimization, as if for the small increase in performance there was a large cost in file size it would not be feasible.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={No changes, LTO, Opt-s, Opt-z, -Os, -Oz, -O, -O3},
                xtick=data,
                x tick label style={rotate=45, anchor=north east, inner sep=0mm},
                bar width = 20,
                enlarge x limits = 0.1,
                ybar,
                ylabel = Size(Kilobytes),
                y label style={at={(axis description cs:-0.01,.5)},anchor=south},
                ymin=0,
                nodes near coords,
                axis x line*=bottom,
                axis y line*=left,
                width=400,
                height=200		]
            \addplot coordinates {
                    (No changes,  1100)
                    (LTO, 920.7)
                    (Opt-s, 954.2)
                    (Opt-z, 979.5)
                    (-Os, 981.5)
                    (-Oz, 981.4)
                    (-O, 981.5)
                    (-O3, 984.2)
                };
        \end{axis}
    \end{tikzpicture}
    \caption{File size of Web Assembly optimizations}
\end{figure}

From the results, the file size isn't greatly affected by the optimization method chosen, however choosing any does provide a reduction in comparison to not choosing any optimizations. However, when it comes to computation time, there is a large range of results, with only Link Time optimization providing an improvement over not making any changes, with the methods designed to provide a smaller file size taking considerably longer, despite not leading to a large difference in file size. Overall, compiling with Link Time Optimization seems to provide an overall better performing program, both when it comes to size and speed.



\subsection{Testing}

For JPEG, as the intensive section is the multiplication of matrices, there aren't properties of the image that lead to one taking longer than another. The exception to this is the overall size of the image as this determines the number of blocks that are created from the image. So for evaluation of the algorithms the size is the only factor to consider when choosing an image.

The first evaluation is to determine the running time of the algorithm. The testing method uses the built-in \texttt{console.time} method in JavaScript to time how long a function takes. This is measured just over the actual function that does the JPEG conversion, rather than the whole process, so get more accurate results about the algorithm itself.

Memory usage is also an important metric, especially on mobile devices that might have small amounts of memory. This can be measured using the \texttt{/usr/bin/time} command on Linux devices, which measures both the time taken and memory usage of a command. This metric is less accurate than the timing evaluation already discussed as it measures the memory usage of the whole command, rather than just of the JPEG conversion. To try and counteract this difference, the code outside the JPEG conversion is kept as similar as possible to ensure it has the same memory consumption.

In order to improve the accuracy of the calculations of the Web Assembly implementation, I have used the \texttt{web\_sys} crate which allows me to run \texttt{console.time} inside my rust code. This displayed a discrepancy of around 100ms between the internal and external time, which is insignificant on some of the longer running programs, but is useful for the shorter running programs.

A full profiler hasn't been used here as there aren't widely used JavaScript profilers that evaluate at the level of how long each function takes. Instead, they focus on things such as response time and CPU utilization of the overall program, which doesn't deliver the results I'd look for.

\subsection{Verification and Validation}

To verify these results, they are compared against a basic conversion using imagemagick, this is a popular command line tool for converting between image formats. Due to this not having the overhead of either Web Assembly or JavaScript it is expected for this to be faster than both implementations, but it allows for getting a rough idea of how long the algorithm should take. As this command does both the decoding from one format and the encoding to another, it is required to also measure my implementations in the same way for verification.

\subsection{Stages of the life cycle undertaken}

The development of this project was done using the Waterfall methodology \cite{royce1987managing}. First the system and software requirements were formed of what I want the application to do. Then doing requirements analysis to determine the sections needed to complete the work. The software architecture is then planned out during the design stage, determining what the system should look like. Then comes the actual coding, producing the system I need. Afterwards I test the system, ensuring the results are what I expect.


\section{Results}

Testing was done on a 24MP, 11 Megabyte, PNG format photo of a deer. The computation was done on an Intel i5 6600 processor with 16 GB of RAM.


The below chart shows the time comparison of the JavaScript and Web Assembly implementations.

\begin{minipage}[t]{0.5\textwidth}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={JavaScript, Web Assembly},
                    xtick=data,
                    bar width = 20,
                    enlarge x limits = 0.5,
                    ybar,
                    ylabel = Time(Seconds),
                    ymin=0,
                    nodes near coords,
                    axis x line*=bottom,
                    axis y line*=left,
                    width=200,
                    legend style={at={(0.5,-0.15)},
                            anchor=north,legend columns=-1}		]
                \addplot coordinates {
                        (JavaScript,   2.48)
                        (Web Assembly,  1.60)
                    };
                \addplot coordinates {
                        (JavaScript,   12)
                        (Web Assembly,  1.14)
                    };
                \legend{JPEG Encoding,Gaussian Blur}
            \end{axis}
        \end{tikzpicture}
        \caption{Algorithm timings}
    \end{figure}
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={JavaScript, Web Assembly},
                    xtick=data,
                    bar width = 20,
                    enlarge x limits = 0.5,
                    ybar,
                    ylabel = Time(Milliseconds),
                    ymin=0,
                    nodes near coords,
                    axis x line*=bottom,
                    axis y line*=left,
                    width=200,
                    legend style={at={(0.5,-0.15)},
                            anchor=north,legend columns=-1}		]
                \addplot coordinates {
                        (JavaScript,   102)
                        (Web Assembly,  77)
                    };
                \addplot coordinates {
                        (JavaScript,   103)
                        (Web Assembly,  278)
                    };
                \legend{Brightness, Contrast}
            \end{axis}
        \end{tikzpicture}
        \caption{Lightweight algorithm timing}
    \end{figure}
\end{minipage}
\\
\\

For the JPEG encoding I measured the time and memory consumption for the overall algorithm, including imagemagick, a command line conversion tool. This measurement was only applicable for JPEG encoding as in every process imagemagick will do image encoding to produce an output, so the measurement wouldn't just be measuring the algorithm in question.


\begin{minipage}[t]{0.45\textwidth}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={JavaScript, Web Assembly, imagemagick},
                    xtick=data,
                    x tick label style={rotate=45, anchor=north east, inner sep=0mm},
                    bar width = 20,
                    enlarge x limits = 0.2,
                    ybar,
                    ylabel = Time(Seconds),
                    ymin=0,
                    nodes near coords,
                    axis x line*=bottom,
                    axis y line*=left,
                    width=200,
                    height=200		]
                \addplot coordinates {
                        (JavaScript,  3.17)
                        (Web Assembly,  2.79)
                        (imagemagick,  0.37)
                    };
            \end{axis}
        \end{tikzpicture}
        \caption{Full process}
    \end{figure}\end{minipage}
\begin{minipage}[t]{0.05\textwidth}
\end{minipage}
\begin{minipage}[t]{0.45\textwidth}
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={JavaScript, Web Assembly, imagemagick},
                    x tick label style={rotate=45, anchor=north east, inner sep=0mm},
                    xtick=data,
                    bar width = 20,
                    enlarge x limits = 0.2,
                    ybar,
                    ylabel = Memory (Megabytes),
                    ymin=0,
                    nodes near coords,
                    axis x line*=bottom,
                    axis y line*=left,
                    width=200,
                    height=200		]
                \addplot coordinates {
                        (JavaScript,  61.4)
                        (Web Assembly,  28.8)
                        (imagemagick,  10.6)
                    };
            \end{axis}
        \end{tikzpicture}
        \caption{Memory consumption of process}
    \end{figure}\end{minipage}



\section{Evaluation}

\subsection{Solution Strengths}

The Web Assembly solution offers significant improvement over JavaScript for complex algorithms, being $8.3 \times$ faster at performing the Gaussian blur task and $1.55 \times$ faster at performing JPEG encoding. For the Gaussian blur, with a processing time of 12 seconds for JavaScript, this takes it above the 10-second limit for keeping the user's attention, as discussed in \cite{nielsen1994usability}, giving Web Assembly a big advantage in making the application feel responsive. These improvements are roughly in line with the results found by fastq.bio in their implementation \cite{fastq}.

The Web Assembly implementation also has much lower memory consumption, which is important for low power devices such as mobile phones with low memory capacity. This is likely due to the increased control the language has for memory management, allowing the programmer to specify how much memory they want and when they don't want it anymore. In contrast, JavaScript handles all the memory management itself, making assumptions about when memory can be freed.

Rust has a big advantage in developer experience for having static types, meaning that the language server is capable of telling the developer what parameters a function takes, and what their types are. This is very useful for debugging as it may be the case that an incorrect type of variable has been passed to a function, in JavaScript this could either lead to unexpected behaviour, or cause an error further down the chain, whereas in Rust, this problem will be immediately highlighted to the user in their IDE. This also makes it easier to remember what functions do without looking at documentation as you can see what parameters they need and what type of data you will get out from it. JavaScript does offer the ability for static types through TypeScript, a superset of the language that includes types, however due to this being an addition to the language, not all packages provide it, so the benefit isn't as large as with Rust.




\subsection{Solution Limitations}

Both the JavaScript and Web Assembly options had significantly worse performance in comparison to the native code of imagemagick, with Web Assembly being $7.5 \times$ slower, which is in line with the results found in the related work section of this paper. There is still significant overhead to running code in websites, and the benefit of the flexibility has to outweigh it.

In addition, while results show that for complex algorithms, Web Assembly does offer a significant benefit over JavaScript, for simple algorithms, it's a trade-off between the two as to which is faster. This is likely due to the additional complexity involved in getting Web Assembly code running in browsers, compared to JavaScript which they have been built to run for many years.

While I did discuss the benefits of static types in the strengths section, this also comes with its downsides in that it makes code much more verbose when having to include types for everything, and requires additional thinking to ensure everything has the correct type. JavaScript also has a much wider range of built-in functions which came in useful when creating the algorithm, such as the \texttt{map} function which condensed the size of the matrix multiplication code.


\section{Conclusions}

In this paper, we have demonstrated Web Assembly offering considerable performance improvements over JavaScript, with it being up to $8\times$ faster. However, we also demonstrated areas where Web Assembly is not suitable, such as when you have the option to run native code or in small low performance tasks. We also evaluated what options to choose when compiling web assembly, finding that Link Time Optimization can provide an improvement both in performance and file size.

One new development that could help with image processing on the web is WebGPU \cite{webgpu}. This is a standard that allows for access of the GPU by both JavaScript and Web Assembly. Due to the nature of image processing algorithms in performing parallel operations, such as in JPEG where much of the computation is done on the $8\times 8$ blocks, each of these can be parallelized. As GPUs are designed for such operations, often having many more cores than a CPU, this could lead to a large improvement in performance.



\newpage
\bibliography{projectpaper}


\end{document}